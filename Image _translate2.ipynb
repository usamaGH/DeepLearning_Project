{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image _translate2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UIlMuW6G_a3-","colab_type":"code","outputId":"5069ee78-bcfa-47e9-d008-f6750d827458","executionInfo":{"status":"ok","timestamp":1561203595970,"user_tz":-120,"elapsed":66316,"user":{"displayName":"OUSSAMA GHAILAN","photoUrl":"","userId":"12246709372421369544"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["##Opening Data And saving\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","!ls '/content/drive/Team Drives/Deep Learning compartido/Proyecto/'\n","myDrive = '/content/drive/Team Drives/Deep Learning compartido/Proyecto/data/'\n","myRoot = '/content/drive/Team Drives/Deep Learning compartido/Proyecto/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","'A Beginner'\\''s Guide to Generative Adversarial Networks (GANs) | Skymind.desktop'\n"," backup\n","'Captura de 2019-06-12 12-46-38.png'\n"," data\n","'Final Project.gslides'\n"," hardikbansal_github_io_CycleGANBlog.pdf\n","'Image _translate2(3).ipynb'\n","'Image _translate2(4).ipynb'\n","'Image _translate2.ipynb'\n","'Image _translate.ipynb'\n"," TRANSFER\n"," ujjwalkarn_me_2016_08_11_intuitive_explanation_convnets.pdf\n"," Untitled0.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8qXSySOiA1BE","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","from torchvision.utils import save_image\n","from torchvision import datasets, transforms\n","\n","\n","from PIL import Image\n","import cv2\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rP0RBRJlpFzs","colab_type":"text"},"source":["**PROGRESS BAR**"]},{"cell_type":"code","metadata":{"id":"6GTEWf1ApD2K","colab_type":"code","colab":{}},"source":["def progressbar(current,limit,nbefore,description,enddescription):\n","  #█\n","  print(\"\\b\"*nbefore)\n","  nbar = 20\n","  bar = int(current*nbar/limit)\n","  cadena = \"[\"+str(current)+\"|\"+str(limit)+\"][\"+\"#\"*(bar)+\" \"*(nbar-bar)+\"]\" + description\n","  ncadena = len(cadena)\n","  print(cadena,end=\"\")\n","  if(current == limit):\n","    print(enddescription)\n","  \n","  return(ncadena+1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fa5uu-3dj2nE","colab_type":"text"},"source":["**TRANSFER LEARNING**"]},{"cell_type":"code","metadata":{"id":"wXHtki6Nj6X5","colab_type":"code","outputId":"35755132-5b5e-4f89-a59d-26bb6745fc18","executionInfo":{"status":"ok","timestamp":1561203609399,"user_tz":-120,"elapsed":79611,"user":{"displayName":"OUSSAMA GHAILAN","photoUrl":"","userId":"12246709372421369544"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import torchvision.models as models\n","resnet50_model = models.resnet50(pretrained=True) #We need pretrained resnet\n","for param in resnet50_model.parameters():\n","  param.requires_grad = False\n","\n","class _Resnet50(nn.Module):\n","            def __init__(self):\n","                super( _Resnet50, self).__init__()\n","                self.features = nn.Sequential(\n","                    *list(resnet50_model.children())[:1] #Taking the firstlayer of pretrained resnet\n","                )\n","            def forward(self, x):\n","                x = self.features(x)\n","                return x\n","\n","model = _Resnet50().cuda()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","100%|██████████| 102502400/102502400 [00:02<00:00, 40730233.31it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"RUIowHCtAYmr","colab_type":"text"},"source":["**Generator**"]},{"cell_type":"code","metadata":{"id":"qz1GK-nJAhaX","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        \n","               \n","        #ENCODER\n","        \n","        self.convE1 = nn.Conv2d(3, 64, 7, 2, 3)     #Use that if no pretrained model are required #128\n","        self.convE2 = nn.Conv2d(64, 128, 3, 2, 1)    #64\n","        self.convE3 = nn.Conv2d(128, 128, 3, 2, 1)   #32\n","        \n","        self.bnE1 = nn.BatchNorm2d(64)\n","        self.bnE2 = nn.BatchNorm2d(128)\n","        self.bnE3 = nn.BatchNorm2d(128)        \n","        \n","        #BOTTLENECK\n","        \n","        self.convT1 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT2 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT3 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT4 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT5 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT6 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT7 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT8 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT9 = nn.Conv2d(128, 128, 5, 1, 2)   #32\n","        self.convT10 = nn.Conv2d(128, 128, 5, 1, 2)  #32\n","        \n","        self.bnT1 = nn.BatchNorm2d(128)\n","        self.bnT2 = nn.BatchNorm2d(128)\n","        self.bnT3 = nn.BatchNorm2d(128)\n","        self.bnT4 = nn.BatchNorm2d(128)\n","        self.bnT5 = nn.BatchNorm2d(128)\n","        self.bnT6 = nn.BatchNorm2d(128)\n","        self.bnT7 = nn.BatchNorm2d(128)\n","        self.bnT8 = nn.BatchNorm2d(128)\n","        self.bnT9 = nn.BatchNorm2d(128)\n","        self.bnT10 = nn.BatchNorm2d(128)\n","        \n","        \n","        \n","        #DECODER\n","        self.convD1 = nn.ConvTranspose2d(128, 128, 4, 2, 1)    #64\n","        self.convD2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)     #128\n","        self.convD3 = nn.ConvTranspose2d(64, 3, 8, 2, 3)       #256\n","        \n","        self.bnD1 = nn.BatchNorm2d(128)\n","        self.bnD2 = nn.BatchNorm2d(64)\n","        self.bnD3 = nn.BatchNorm2d(3)\n","\n","        self.sig = nn.Sigmoid()\n","\n","\n","    def forward(self, x):\n","        \n","        out = model(x) #Using the first layer of resnet pretrained model for clasification\n","        #out = self.relu(self.bnE1(self.convE1(x)))  #Use that if no pretrained model are required\n","        out = self.sig(self.bnE2(self.convE2(out)))\n","        out = self.sig(self.bnE3(self.convE3(out)))\n","        \n","        #INTERNAL RESNET LAYER\n","        out_ = self.sig(self.bnT1(self.convT1(out)))\n","        out = out_ + out\n","        out_ = self.sig(self.bnT2(self.convT2(out)))\n","        out = out_ + out\n","        out_ = self.sig(self.bnT3(self.convT3(out)))        \n","        out = out_ + out\n","        out_ = self.sig(self.bnT4(self.convT4(out)))\n","        out = out_ + out\n","        out_ = self.sig(self.bnT5(self.convT5(out)))\n","        out = out_ + out\n","        out_ = self.sig(self.bnT6(self.convT6(out)))\n","        out = out_ + out\n","        \n","        #DECODING IMAGE\n","        out = self.sig(self.bnD1(self.convD1(out)))\n","        out = self.sig(self.bnD2(self.convD2(out)))\n","        out = self.sig(self.bnD3(self.convD3(out)))\n","\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMyNL6U5Sudn","colab_type":"text"},"source":["**Discriminator**"]},{"cell_type":"code","metadata":{"id":"ITCUOAjGStp7","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        \n","        \n","        #TRANSFER FEATURE EXTRACTION\n","        #DECISION CONVOLUTION\n","        \n","        self.sig = nn.Sigmoid()\n","        \n","        \n","        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)      #Use that if not pretrained one is required#128\n","        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)     #64\n","        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)    #32\n","        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)    #16\n","        self.conv5 = nn.Conv2d(512, 1, 3, 1, 1)      #16\n","        \n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(128)\n","        self.bn3 = nn.BatchNorm2d(256)\n","        self.bn4 = nn.BatchNorm2d(512)\n","        self.bn5 = nn.BatchNorm2d(1)\n","        \n","        self.fc = nn.Linear(16*16,1)\n","\n","        \n","\n","    def forward(self, x):\n","        \n","        out = model(x) #Using the first layer of resnet pretrained model for clasification\n","        #out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.sig(self.bn2(self.conv2(out)))\n","        out = self.sig(self.bn3(self.conv3(out)))\n","        out = self.sig(self.bn4(self.conv4(out)))\n","        out = self.sig(self.bn5(self.conv5(out)))\n","        \n","        \n","        out = self.fc(out.view(out.size()[0],-1))\n","        out = self.sig(out)\n","        \n","        return out\n","    \n","    def hook(self, module, input, output):\n","        self.outputs[0]  = output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa5HDuGwp8JV","colab_type":"text"},"source":["**DATASET TRANSFORMATION**"]},{"cell_type":"code","metadata":{"id":"MGu_pHIEqDId","colab_type":"code","outputId":"7fc5d3f4-9517-40af-ef24-d64384cd986e","executionInfo":{"status":"ok","timestamp":1561206390687,"user_tz":-120,"elapsed":507,"user":{"displayName":"OUSSAMA GHAILAN","photoUrl":"","userId":"12246709372421369544"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","#Taking the dataset with mini batches and aplying the transformation\n","BATCH = 8\n","trans = transforms.ToPILImage()\n","\n","data_transform = transforms.Compose([\n","        transforms.Resize((256,256)),\n","        transforms.ToTensor()\n","])\n","\n","datasetB = datasets.ImageFolder(root=myDrive+'DATASETS/RETRATOS/',\n","                                           transform=data_transform)\n","dataset_loaderB = torch.utils.data.DataLoader(datasetB,\n","                                             batch_size=BATCH, shuffle=True,\n","                                             num_workers=8)\n","\n","datasetA = datasets.ImageFolder(root=myDrive+'DATASETS/FOTOS/',\n","                                           transform=data_transform)\n","dataset_loaderA = torch.utils.data.DataLoader(datasetA,\n","                                             batch_size=BATCH, shuffle=True,\n","                                             num_workers=8)\n","\n","print(len(datasetB))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["606\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2ETZz9tjprdo","colab_type":"text"},"source":["**TRAIN**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hlWeD6bCbZs7","outputId":"07bd3c03-13de-4b0c-aecf-5aa3cdf03d33","executionInfo":{"status":"error","timestamp":1560613179164,"user_tz":-120,"elapsed":924698,"user":{"displayName":"OUSSAMA GHAILAN","photoUrl":"","userId":"12246709372421369544"}},"colab":{"base_uri":"https://localhost:8080/","height":12695,"output_embedded_package_id":"131jWBolt-LDnXMHZJXXYxEjb53fBiJg0"}},"source":["import torchvision.models as models\n","\n","G_AB = Generator().cuda()\n","G_BA = Generator().cuda()\n","DA = Discriminator().cuda()\n","DB = Discriminator().cuda()\n","\n","#--------------------Load our Trained Model-----------------------------\n","G_AB.load_state_dict(torch.load(myDrive+'DATASETS/MODEL/G_AB3.pth'))\n","G_AB.eval()\n","G_BA.load_state_dict(torch.load(myDrive+'DATASETS/MODEL/G_BA3.pth'))\n","G_BA.eval()\n","DA.load_state_dict(torch.load(myDrive+'DATASETS/MODEL/DA3.pth'))\n","DA.eval()\n","DB.load_state_dict(torch.load(myDrive+'DATASETS/MODEL/DB3.pth'))\n","DB.eval()\n","#--------------------/Load our Trained Model-----------------------------\n","\n","\n","optimizer_G_AB = optim.Adam(G_AB.parameters(), lr=0.0002)\n","optimizer_G_BA = optim.Adam(G_BA.parameters(), lr=0.0002)\n","optimizer_DA = optim.Adam(DA.parameters(), lr=0.0002)\n","optimizer_DB = optim.Adam(DB.parameters(), lr=0.0002)\n","\n","\n","criterion = nn.MSELoss()\n","criterion_D = nn.BCELoss()\n","\n","\n","#Realy = Variable(torch.ones(4,1)).cuda()\n","#Fakey = Variable(torch.zeros(4,1)).cuda()\n","#label = torch.cat((Realy,Fakey),0)\n","\n","alfa = 10 #for cycle loss \n","beta = 0.01  #for indentity loss\n","epochs = 500\n","\n","#Creating a dictionary of losses for tracking the model \n","loss = {\"cycLoss\":[],\"G_AB_Loss\":[],\"G_BA_Loss\":[],\"DB_Loss\":[],\"DA_Loss\":[]}\n","\n","print(\"Training ...\")\n","\n","\n","for epoch_idx in range(epochs):\n","  nbefore = 0  #Parameter for the progressbar status\n","  processed_batches = 1   #Parameter for the progressbar status\n","  for b,a in zip(dataset_loaderB,dataset_loaderA):\n","    #Dataset length of both domain are diferent so we need continue the iteration until we reach max data\n","    if(b[0].size()[0] != a[0].size()[0]):\n","      continue\n","    tmp = [a[0],b[0]]\n","    G_AB.train()\n","    G_BA.train()\n","    DB.train()\n","    DA.train()\n","    \n","    b = b[0].cuda()\n","    a = a[0].cuda()\n","\n","    #First Cycle\n","    outG_AB_cycA = G_AB(a)\n","    outDB_cycA_Real = DB(b)\n","    outDB_cycA_Fake = DB(outG_AB_cycA)\n","    outG_BA_cycA = G_BA(outG_AB_cycA)\n","    #Second Cycle\n","    outG_BA_cycB = G_BA(b)\n","    outDA_cycB_Real = DA(a)\n","    outDA_cycB_Fake = DA(outG_BA_cycB)\n","    outG_AB_cycB = G_AB(outG_BA_cycB)\n","    \n","    #Make the identity constraint for avoiding full noise output\n","    same_B = G_BA(b)\n","    loss_identity_B = criterion(same_B, b)*beta\n","    same_A = G_AB(a)\n","    loss_identity_A = criterion(same_A, a)*beta\n","    \n","    #Build a labels for the discriminator loss function\n","    Realy = Variable(torch.ones(b.size()[0],1)).cuda()\n","    Fakey = Variable(torch.zeros(b.size()[0],1)).cuda()\n","    \n","    #Preparing loss functions\n","    cycLoss = criterion(outG_BA_cycA,a) + criterion(outG_AB_cycB,b)\n","    DB_Loss = ( criterion_D(outDB_cycA_Real,Realy) + criterion_D(outDB_cycA_Fake,Fakey) )*0.5\n","    DA_Loss = ( criterion_D(outDA_cycB_Real,Realy) + criterion_D(outDA_cycB_Fake,Fakey) )*0.5\n","    G_AB_Loss = criterion(outDB_cycA_Fake,Realy) + alfa*cycLoss\n","    G_BA_Loss = criterion(outDA_cycB_Fake,Realy) + alfa*cycLoss\n","    Gtot = loss_identity_B + loss_identity_A + criterion(outDA_cycB_Fake,Realy) + criterion(outDB_cycA_Fake,Realy) + alfa*cycLoss\n","    \n","    #Update network weights\n","    optimizer_G_AB.zero_grad()\n","    optimizer_G_BA.zero_grad()\n","    Gtot.backward(retain_graph=True)\n","    optimizer_G_BA.step()\n","    optimizer_G_AB.step()\n","    \n","    optimizer_DA.zero_grad()\n","    DA_Loss.backward()\n","    optimizer_DA.step()\n","    \n","    optimizer_DB.zero_grad()\n","    DB_Loss.backward()\n","    optimizer_DB.step() \n","    \n","    #Saving losses of each iteration to plot it\n","    loss[\"cycLoss\"].append(cycLoss.item())\n","    loss[\"G_AB_Loss\"].append(G_AB_Loss.item()) \n","    loss[\"G_BA_Loss\"].append(G_BA_Loss.item()) \n","    loss[\"DB_Loss\"].append(DA_Loss.item()) \n","    loss[\"DA_Loss\"].append(DB_Loss.item()) \n","    \n","    #Relevant info to show while network are training\n","    loss_info = \"  Loss -> cycLoss:\"+str(cycLoss.item())+\" ,G_AB:\"+str(G_AB_Loss.item())+\" ,G_BA:\"+str(G_BA_Loss.item())+\" ,DA:\"+str(DA_Loss.item())+\" ,DB:\"+str(DB_Loss.item())\n","    #Update progress bar to see status of the network training process\n","    nbefore = progressbar(processed_batches,int(len(datasetB)/BATCH),nbefore,\"\",\"Epoch: \"+str(epoch_idx)+loss_info) \n","    processed_batches += 1\n"," \n","\n","  #After each epoch saves the parameters learned\n","  torch.save(G_AB.state_dict(), myDrive+'DATASETS/MODEL/G_AB3.pth')\n","  torch.save(G_BA.state_dict(), myDrive+'DATASETS/MODEL/G_BA3.pth')\n","  torch.save(DA.state_dict(), myDrive+'DATASETS/MODEL/DA3.pth')\n","  torch.save(DB.state_dict(), myDrive+'DATASETS/MODEL/DB3.pth')\n","  \n","  #Each epoch multiple of 10 are ploted with the first image and a plot showing the losses during that period\n","  if( (epoch_idx%10) == 0 ):\n","    #All the following code are only for data representation\n","    print('\\n=============== EPOCH {} ================='.format(epoch_idx))\n","\n","    print(\"CYC_LOSS: \",cycLoss)\n","    print(\"GAB_LOSS: \",G_AB_Loss)\n","    print(\"GBA_LOSS: \",G_BA_Loss)\n","    print(\"DB_LOSS: \",DB_Loss)\n","    print(\"DA_LOSS: \",DA_Loss)\n","\n","    \n","    plt.figure()\n","    plt.subplot(231)\n","    plt.imshow(trans(tmp[0][0].detach().cpu()))\n","    plt.subplot(232)\n","    plt.imshow(trans(outG_AB_cycA[0].detach().cpu()))\n","    plt.subplot(233)\n","    plt.imshow(trans(outG_BA_cycA[0].detach().cpu()))\n","    plt.subplot(234)\n","    plt.imshow(trans(tmp[1][0].detach().cpu()))\n","    plt.subplot(235)\n","    plt.imshow(trans(outG_BA_cycB[0].detach().cpu()))\n","    plt.subplot(236)\n","    plt.imshow(trans(outG_AB_cycB[0].detach().cpu()))\n","    plt.figure()\n","    plt.title(\"Loss epoch \"+str(epoch_idx))\n","    plt.plot(loss[\"cycLoss\"],label=\"cycLoss\")\n","    plt.plot(loss[\"G_AB_Loss\"],label=\"G_AB_Loss\")\n","    plt.plot(loss[\"G_BA_Loss\"],label=\"G_BA_Loss\")\n","    plt.plot(loss[\"DB_Loss\"],label=\"DB_Loss\")\n","    plt.plot(loss[\"DA_Loss\"],label=\"DA_Loss\")\n","    plt.legend()\n","    plt.show()\n","    del loss\n","    loss = {\"cycLoss\":[],\"G_AB_Loss\":[],\"G_BA_Loss\":[],\"DB_Loss\":[],\"DA_Loss\":[]}\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"bwsbPURapJ8a","colab_type":"code","colab":{}},"source":["\"\""],"execution_count":0,"outputs":[]}]}